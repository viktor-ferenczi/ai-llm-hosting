#!/bin/bash
# LLM selection

# Models directory
export MODELS_DIR="$HOME/models"

# API: api, oai, outlines
export LLM_API="outlines"

# Model: llama3-70b, codellama-13b, codellama-34b, codellama-70b, phind-codellama-34b, deepseek-coder-6.7b, deepseek-coder-33b, codefuse-deepseek-coder-33b, deepseek-llm-67b, yi-6b, yi-34b, lwm-text-chat, mixtral, starcoder2-15b
export LLM_MODEL="llama3-70b"

# Logging
export LLM_LOG_PATH="$HOME/log/llm-engine.log"

# Scripts
export LLM_RUN_SCRIPT="run-$LLM_API-$LLM_MODEL"

# Outlines source folder
export OUTLINES_SOURCE_DIR="$HOME/dep/outlines-contrib"

# API server module
case "$LLM_API" in
    api)
        export LLM_API_SERVER_MODULE='vllm.entrypoints.api_server'
        ;;
    oai)
        export LLM_API_SERVER_MODULE='vllm.entrypoints.openai.api_server'
        ;;
    outlines)
        export LLM_API_SERVER_MODULE='outlines.serve.serve'
        ;;
    *)
        echo "Invalid LLM_API: $LLM_API"
        exit 1
        ;;
esac
